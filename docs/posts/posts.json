[
  {
    "path": "posts/2022-05-06-reproducibility-on-day-16/",
    "title": "Reproducibility on Day 16",
    "description": "Reproducibility of experiments, observational studies and reports is key.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-05-06",
    "categories": [
      "Data analysis",
      "Data science",
      "Reproducibility",
      "Discussion"
    ],
    "contents": "\r\nReproducibility, as I was taught in undergrad, is making sure your experiments, observational studies and reports can be replicated by other people, either to repeat your processes or to expand upon them. The key points to reproducibility are listed below, and will be expanded upon:\r\nProviding the data set used in an experiment, observational study or report.\r\nProviding the instructions for cleaning the data set, whether the data set provided is the cleaned data set or the original data set.\r\nProviding the Rmd file that was used to knit the PDF report.\r\nProviding the data set used in an experiment, observational study or report\r\nIn order for someone else to be able to replicate your findings, they will need to use the same data that you did. There are several layers and things to be aware of here. First, if the data was created exclusively in an R or Rmd file (i.e. was not obtained externally via a CSV file, for example) then you need to make sure the data can be replicated if it is simulated data. In this case, you would need to set a seed, so that if someone replicates your work, they will get the same data when simulating the data. Second, if the data is obtained externally, then you need to make sure it is data that can be shared publicly. This means looking up the licensing stipulations for the data, whether it’s open data or not, or if the data set contains highly sensitive information that can be used to de-anonymize users. It is always best (and ethical) to strive to share publicly available open data, which typically have licenses that permit sharing with attribution to the source. If the data contains sensitive information, that information should either be further anonymized or removed entirely from the data set for privacy considerations. In this case, if you are permitted to share the data set, you should only share the cleaned version of the data set with sensitive information either modified or removed, and then share the steps taken to clean the data so that other people can download the original data set and follow your cleaning instructions in order to obtain your cleaned data set themselves if they so wish.\r\nProviding the instructions for cleaning the data set, whether the data set provided is the cleaned data set or the original data set\r\nThe data you are working with will most likely need to be cleaned, either to remove or modify any sensitive information, rename variables, produce new variables, and so on. Thus, it is crucial that all these steps be documented in detail so that someone else can clean the original data set and obtain the cleaned data set that you are using in your report. The level of detail is important here, because the more detailed your explanation is, the easier it will be for someone else to follow along.\r\nProviding the Rmd file that was used to knit the PDF report\r\nProviding an Rmd file not only gives someone access to all the code used to conduct the data analysis, but it also allows people to reproduce your report in its entirety. This serves as a verification that your report is fully based on the results obtained from your code.\r\nReproducibility is key to verifying and expanding upon results from data analyses, so it is important that reproducibility be embraced more and more in the fields of data analysis and data science. It benefits us all.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-06T23:38:36-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-05-bilingualism-on-day-15/",
    "title": "Bilingualism on Day 15",
    "description": "I have my parents to thank for being fully bilingual in English and Spanish.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-05-05",
    "categories": [
      "Language",
      "Life",
      "Discussion"
    ],
    "contents": "\r\nAs those who know me personally know (and probably my Twitter followers as well), I am fully bilingual in English and Spanish. I was born in Colombia and lived there for around five years of my life before we packed up our bags and moved to Toronto in Canada. During my brief time in Colombia, I started picking up the basics of Spanish, up until pre-kindergarten. We then moved to Canada, and I was directly enrolled into kindergarten. I quickly embraced English, and was soon on par with the other kids. This was good news, but my parents were worried that I would start to lose my Spanish. So they only spoke to me in Spanish at home, and also enrolled me in Saturday Spanish classes for some time. Thanks to my parents’ efforts, I thrived in Spanish as well, and I am proud to be fully bilingual in English and Spanish. Through elementary and high school classes I also learned some French, but today I am only at a beginner level, only able to hold a very basic conversation in French. One of my goals is to bring up my French fluency to the point where I can claim I am either intermediate or fully fluent in French. In today’s world, bilingualism / multilingualism is very beneficial, and I am fortunate that this was reinforced into me by my parents. There are many programs available, as well as courses in schools and universities, that allow anyone to become fluent in another language. If you get the chance, I highly recommend learning another language.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T20:04:48-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-04-correlation-does-not-equal-causation-on-day-14/",
    "title": "Correlation does not equal causation on Day 14",
    "description": "A brief post on the difference between correlation and causation, and why this is important.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-05-04",
    "categories": [
      "Statistics",
      "Correlation",
      "Causation"
    ],
    "contents": "\r\nOne idea that was drilled into our heads in an observational study course in undergrad was the idea that correlation does not equal causation. And thankfully it was, for the distinction between correlation and causation is incredibly important not just in the field of statistics, but also in everyday life.\r\nI’ll set out a fun scenario for you. Let’s say there’s a fan of the Toronto Maple Leafs that puts on a new necklace before every game as a good luck charm for the team to win. This enthusiastic fan has put on six necklaces, and the Leafs have won all six games that they have played. If we were to plot this data out in a scatter plot, we can see a clear positive correlation occur between the number of necklaces worn and the number of games the Leafs have won up until that point. You know what, let’s plot this out!\r\n\r\n\r\n# Create vector for number of necklaces worn.\r\nnum_necklaces <- c(1:6)\r\n\r\n# Create vector for number of games won by the Leafs up until that point.\r\nnum_games_won <- c(1:6)\r\n\r\n# Create a simple scatter plot, with the x-axis for the number of necklaces\r\n# worn, and the y-axis for the number of games won up until that point.\r\nplot(x = num_necklaces,\r\n     y = num_games_won,\r\n     xlab = \"Number of necklaces worn\",\r\n     ylab = \"Number of games won by Leafs\")\r\ntitle(main = \"Example of correlation\")\r\n\r\n\r\n\r\n\r\nThe graph also makes this correlation very clear. Now, the question is, is there also a causal relationship between the number of necklaces worn and the number of games won by the Leafs up until that point? Let’s think about causality this way: Does the Leafs fan have a direct impact in the team’s success (winning the game) by adding on a new necklace each time the team is set to play? The answer is no, the necklace does not impact the direct outcome of the game, it is just a coincidence that the team wins every time the fan adds on a new necklace. Thus, for this data, we can only say that there is a strong positive correlation between our two variables, but we cannot infer causality between them.\r\nSo remember, correlation does not necessarily imply causation. There will be cases where data is correlated and there is a causal relationship between them, but this is not a general rule.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-04-correlation-does-not-equal-causation-on-day-14/correlation-does-not-equal-causation-on-day-14_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-05-04T23:18:18-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-03-may-showers-on-day-13/",
    "title": "May showers on Day 13",
    "description": "A rainy day for organizing the brain.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-05-03",
    "categories": [
      "Life"
    ],
    "contents": "\r\nOne of the things I love about spring is the constant rain showers. There’s something calming and soothing about the rain, especially when one can be inside, staring out through a window. I took the chance to have a calm day by dedicating myself to organizing my thoughts and plans for the near-future. This included mapping out my career goals and what I need to do to achieve them. I have also set myself up to continue learning French, taking online courses to boost my skills, and taking up more leisure reading. I am also striving to set aside a part of my day for meditation. Rainy days truly are wonderful days.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-03T22:53:26-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-02-not-just-r-part-2-on-day-12/",
    "title": "Not Just R: Part 2 on Day 12",
    "description": "Creating a data frame using Pandas in Python.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-05-02",
    "categories": [
      "Python"
    ],
    "contents": "\r\nWelcome to the second part of my “Not Just R” series of blog posts! These posts will highlight my knowledge of Python in bits and pieces, hence why this is a multi-part series. In today’s entry, we will import the Pandas library in Python, which gives us the tools we need to create a data frame. Let’s do this!\r\nBefore we go ahead and import Pandas, we first need to run the R package called Reticulate, which allows us to run Python in RStudio, where Python has been locally installed on my machine (using Anaconda).\r\n\r\n\r\n# Load in reticulate package in order to run Python in RStudio.\r\nlibrary(reticulate)\r\n\r\n\r\n\r\nNow we should be ready to go with Python! Let’s import Pandas, using the agreed-upon abbreviation pd in order to make our function calls more simple.\r\n\r\n# Import Pandas using pd abbreviation.\r\nimport pandas as pd\r\n\r\nWith Pandas functions now available to us, we can create our data frame. We’ll create a simple one, which stores a person’s name, how many cups of water they drank in one day, and how many hours of sleep they got the night prior. First, we will create a dictionary where the keys will be the column names, and the values will be the column entries.\r\n\r\n# Create the dictionary, with keys being column names, and values being entries.\r\ntable1 = {\"Name\": [\"David\", \"Sam\", \"Marketa\", \"Rana\"],\r\n          \"Cups of Water\": [4, 3, 5, 8],\r\n          \"Hours of Sleep\": [7, 7, 6, 8]}\r\n          \r\n# Convert dictionary into a data frame using Pandas.\r\ndf = pd.DataFrame(table1)\r\n\r\n# Display data frame.\r\ndf\r\n      Name  Cups of Water  Hours of Sleep\r\n0    David              4               7\r\n1      Sam              3               7\r\n2  Marketa              5               6\r\n3     Rana              8               8\r\n\r\nOur data frame has been created, and now we can manipulate it to show us relevant information. For example, say we only want to focus on the number of hours of sleep, regardless of which person they correspond to. We can create a new data frame where we remove all columns except for the “Hours of Sleep” column.\r\n\r\n# Using our original data frame, create a new one with only \"Hours of Sleep\" column.\r\ndf2 = df[[\"Hours of Sleep\"]]\r\n\r\n# Display data frame.\r\ndf2\r\n   Hours of Sleep\r\n0               7\r\n1               7\r\n2               6\r\n3               8\r\n\r\nWe can also create a new data frame where we only keep the rows where the person drank at least 5 cups of water.\r\n\r\n# Select only the rows where the cups of water is at least 5, and create new\r\n# data frame with only the selected rows.\r\ndf3 = df[df[\"Cups of Water\"] >= 5]\r\n\r\n# Display data frame.\r\ndf3\r\n      Name  Cups of Water  Hours of Sleep\r\n2  Marketa              5               6\r\n3     Rana              8               8\r\n\r\nFinally, we will create an array with the unique hours of sleep.\r\n\r\n# Create an array with the unique values for the number of hours of sleep.\r\ndf[\"Hours of Sleep\"].unique()\r\narray([7, 6, 8], dtype=int64)\r\n\r\nAnd that’s it for Part 2 of “Not Just R”! Next time, we’ll take NumPy out for a spin. NumPy is Python’s library for matrix manipulations and is particularly helpful in machine learning applications.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-02T23:24:11-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-01-a-day-to-recharge-on-day-11/",
    "title": "A day to recharge on Day 11",
    "description": "A Sunday for relaxation and reflection.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-05-01",
    "categories": [
      "Life"
    ],
    "contents": "\r\nSundays for me are all about taking a day to unwind and refresh myself, both physically and mentally, for the week ahead. While this is not always the case, especially when it’s crunch time and it seems like I am on 24/7 the entire week, I have been trying to make it a habit. A refreshed mind works much better than a stressed mind. Tasks are much more manageable, and there is more energy to complete them. If you can, do take a day, or even some moments if a full day isn’t possible, to sit back and relax, reflecting on the week that was, engaging in whatever form(s) of self-care that you prefer, and then plan for the week ahead. This will hopefully put you in a better state of mind, and will give you the energy to work on your goals or tasks at hand.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-01T22:06:12-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-30-not-just-r-part-1-on-day-10/",
    "title": "Not Just R: Part 1 on Day 10",
    "description": "Putting the spotlight on Python.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-30",
    "categories": [
      "Python"
    ],
    "contents": "\r\nFrom my undergrad coursework, as well as through some online courses, I have worked at building up my R proficiency, to the point where I am very comfortable using it as my go-to programming language for analysis and basic programming. Virtually all the assignments I did for my undergrad courses that are shared on this website were written using R code.\r\nHowever, R is not the only programming language I know. I have also been exposed to Python programming, both through some undergrad courses and through online courses. But, since I tend to conduct analyses using R, I don’t get to practice using my Python skills as much. I am hoping to change that through both this blog and this website as a whole. Like I am doing with some blog posts where I set out a coding example using R, I would also like to do the same using Python. And that is made easier by the fact that Python code can be written and executed using RStudio, even alongside R code if you wish. I am also planning on completing some personal projects using Python, but that is still in the planning stages.\r\nBasically, even though R is the standard when it comes to programming languages with a statistical analysis focus, Python has also shone in the data science world, and it is vital to continue to develop proficiency in both programming languages. So keep on the lookout for an upcoming Part 2 where I introduce a Python coding example!\r\nBut, as a little teaser, and in homage to what most, if not all, Python beginners experience when starting to learn the language, we will print out the phrase “Hello World!”\r\nIn order to run Python code in RStudio, aside from already having a local installation of Python on your machine (I use Anaconda myself), you need the Reticulate R package. If you don’t have it already installed, please do so before attempting to run the following code.\r\n\r\n\r\nlibrary(reticulate)\r\n\r\n\r\n\r\nNow, we can write Python code as if we were in any other Python IDE! Let’s print “Hello World!”\r\n\r\nprint(\"Hello World!\")\r\nHello World!\r\n\r\nWe did it! That’s just a tiny taste of the Python code we’ll go through in future installments of “Not Just R”. Next time we’ll do some calculations, and maybe even explore Python libraries, which are equivalent to R packages in the sense that they extend the capabilities of base Python, sometimes for more specialized topics.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-30T20:56:40-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-29-a-sunny-day-9-for-website-updates/",
    "title": "A sunny Day 9 for website updates",
    "description": "Building out the \"Beyond the Data\" section.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-29",
    "categories": [
      "Website updates"
    ],
    "contents": "\r\n (Photo by Lisa Fotios: https://www.pexels.com/photo/hand-under-sun-and-tree-1198109/)\r\nThe sunshine has finally come our way here in Toronto, and along with it are more updates to the website. The “Beyond the Data” section has been filled out, and images should be added soon to give the page a bit more flair. It’s been a fun section to work on, because I get to highlight parts of my life that may not be apparent from a resume. Completing this section also means that all the major sections of the website now have content! This website will continually be updated, with new blog and Tidy Tuesday entries, educational projects that I still have to modify to publicly share them, other projects (including one currently in the works!), and updates to my resume as they come.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-29-a-sunny-day-9-for-website-updates/pexels-lisa-fotios-1198109.jpg",
    "last_modified": "2022-04-29T15:57:03-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-28-a-lovely-day-8-for-regression-analysis/",
    "title": "A lovely Day 8 for regression analysis",
    "description": "Practicing regression analysis using fictional data.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-28",
    "categories": [
      "Regression",
      "Statistics",
      "Data analysis",
      "Data visualization"
    ],
    "contents": "\r\nFor today’s blog post on this sunny Thursday in Toronto, I thought it would be great to get in some practice with regression analysis! What we’ll be doing is the following: create fictional data that approximates a linear trend, visualize the data, fit a simple linear regression model to the data, visualize this fit using a line of best fit on our data visual, and make predictions using the model. Let’s do this!\r\nFirst, load in any libraries that are necessary to run our code. Whether or not I use it, I always find it to be good practice to load in the Tidyverse, since it is such a prominent collection of tidy libraries when it comes to data cleaning, analysis and visualization. I also find it a best practice to set a seed for reproducibility of random data; it may not be necessary in this case, but I like to set one just in case.\r\n\r\n\r\n# Load in necessary library.\r\nlibrary(tidyverse)\r\n\r\n# Set seed.\r\nset.seed(314159)\r\n\r\n\r\n\r\nBy this point in the blog, you should start to notice that I have a particularly favourite seed number (and I do love pie, mmm).\r\nNow, with preliminaries out of the way, we can shift our focus to the data. For today’s practice, let’s say we want to predict a student’s grade for the final exam in a particular course, let’s say Economics 101, based on the amount of time in hours that they studied. The data we will record is thus a student’s grade percentage on the final exam, and their corresponding number of hours that they spent studying for the final exam. Let’s assume we recorded the grades for 20 students.\r\nFor the sake of simplicity, the data will be recorded in order from the lowest grade achieved on the final exam, to the highest grade.\r\n\r\n\r\n# Create a student id vector from one to 20.\r\nstudent_id <- c(1:20)\r\n\r\n# Create a vector for the final exam grades for 20 students in Economics 101,\r\n# ordered from the lowest grade to the highest grade.\r\nfinal_grades <- c(37, 47.5, 53.5, 57.5, 58, 60, 63.5, 64, 64, 65,\r\n                  67, 69, 72, 72.5, 72.5, 77, 81, 85, 92, 95.5)\r\n\r\n# Create a vector for the number of hours the 20 students from Economics 101\r\n# studied for the final exam. These hours correspond positionally to the\r\n# grades entered in the final_grades vector.\r\nhours_studied <- c(0, 1, 1, 2, 3, 5, 5, 4, 4, 5,\r\n                   7, 5, 7, 8, 10, 8, 9, 9, 11, 12)\r\n\r\n\r\n\r\nThis data would be nicer in a tibble (see, the Tidyverse always comes in handy!). Let’s make it so!\r\n\r\n\r\n# Store final_grades and hours_studied as variables in a tibble.\r\ngrades_hours_tibble <- tibble(student_id, final_grades, hours_studied)\r\n\r\n# View tibble as a kable.\r\ngrades_hours_kable <- \r\n  knitr::kable(grades_hours_tibble,\r\n               format = \"pipe\",\r\n               col.names = c(\"Student ID\", \"Final exam grades (%)\",\r\n                             \"Number of hours studied\"),\r\n               caption = \"Final exam grades (in %) and number of hours studied for students in Economics 101\")\r\ngrades_hours_kable\r\n\r\n\r\nTable 1: Final exam grades (in %) and number of hours studied for students in Economics 101\r\nStudent ID\r\nFinal exam grades (%)\r\nNumber of hours studied\r\n1\r\n37.0\r\n0\r\n2\r\n47.5\r\n1\r\n3\r\n53.5\r\n1\r\n4\r\n57.5\r\n2\r\n5\r\n58.0\r\n3\r\n6\r\n60.0\r\n5\r\n7\r\n63.5\r\n5\r\n8\r\n64.0\r\n4\r\n9\r\n64.0\r\n4\r\n10\r\n65.0\r\n5\r\n11\r\n67.0\r\n7\r\n12\r\n69.0\r\n5\r\n13\r\n72.0\r\n7\r\n14\r\n72.5\r\n8\r\n15\r\n72.5\r\n10\r\n16\r\n77.0\r\n8\r\n17\r\n81.0\r\n9\r\n18\r\n85.0\r\n9\r\n19\r\n92.0\r\n11\r\n20\r\n95.5\r\n12\r\n\r\nFrom the fictional data, we can clearly see that there seems to be a strong correlation between the final exam grade a student received in Economics 101, and the number of hours they spent studying for the final exam. That is, the lower the number of hours they studied, the lower their mark will be, and conversely, the higher the number of hours they studied, the higher their mark will be. We want to be more rigorous about this, so we will now visualize the data to see if we can visually spot this linear correlation.\r\n\r\n\r\n# Create a scatter plot of the data.\r\ngrades_hours_plot <-\r\n  grades_hours_tibble %>%\r\n  ggplot(aes(x = hours_studied, y = final_grades)) +\r\n  geom_point(colour = \"blue\") +\r\n  labs(title = \"Grades vs. hours studied for Economics 101 final exam\",\r\n       x = \"Number of hours studied\",\r\n       y = \"Final exam grade (in %)\",\r\n       caption = \"Scatter plot.\") +\r\n  theme_minimal()\r\ngrades_hours_plot\r\n\r\n\r\n\r\n\r\nThe plot makes it visually clear that there seems to be a positive linear correlation between the grade a student received in the Economics 101 final exam and the number of hours that they studied for the final exam.\r\nNow, we can fit a simple linear regression model to the data, since we claim that this model will give us the most approximate fit to the data.\r\n\r\n\r\n# Fit data to simple linear regression model.\r\ngrades_hours_model <-\r\n  lm(final_grades ~ hours_studied, data = grades_hours_tibble)\r\n\r\n# Get summary of model.\r\nsummary(grades_hours_model)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = final_grades ~ hours_studied, data = grades_hours_tibble)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-11.5387  -2.0418   0.8055   3.7680   4.8574 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)     45.078      2.113   21.33 3.15e-14 ***\r\nhours_studied    3.896      0.315   12.37 3.10e-10 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 4.748 on 18 degrees of freedom\r\nMultiple R-squared:  0.8947,    Adjusted R-squared:  0.8889 \r\nF-statistic:   153 on 1 and 18 DF,  p-value: 3.095e-10\r\n\r\nAs we can see from the raw output we obtain by using the summary() function on our model, the estimate for \\(\\hat{\\beta}_1\\), which is the estimate for the slope coefficient, is 3.896, which means that for every additional hour of study, the predicted grade will increase by 3.896 percentage points. \\(\\hat{\\beta}_1\\) also has a p-value of approximately 0. At a significance level of \\(\\alpha\\) = 0.05, this means that the number of hours studied is significant in estimating the grade a student will receive on the final exam. We also have the estimate for \\(\\hat{\\beta}_0\\) (estimate for the intercept), which is 45.077, which means that if a student doesn’t study at all, their grade is estimated as 45.078%. The adjusted R-squared value is 0.889, which means that we can have great confidence in the predictions made by our model, since the closer the adjusted R-squared value is to 1, the more reliable our model is at making predictions.\r\nIn summary, we can rely on our model to make predictions, and we have the coefficient estimates necessary to write an equation for predicting grades based on the number of hours spent studying. That equation is the following:\r\n\\[\\hat{y} = 45.078 + 3.896x\\]\r\nLet’s test it out! Consider a student that studies for eight hours. What is their predicted grade on the Economics 101 final exam?\r\n\r\n\r\n# Predict final exam grade for student that studies for eight hours.\r\npredicted_grade <- 45.078 + (3.896 * 8)\r\n\r\n# Print the predicted grade.\r\npredicted_grade\r\n\r\n\r\n[1] 76.246\r\n\r\nThus, using our model, we predict that the student will receive a mark of approximately 76.25% on their Economics 101 final exam.\r\nNow, let’s be very clear. This is a simplified model that does not take into account any other external factors that can affect a student’s performance on a final exam, such as stress, physical and mental health, etc. Thus, just because our model predicts a student will get a mark of 76.25% does not mean this will always be true.\r\nFinally, we will see how close our model is to our actual data using a visualization.\r\n\r\n\r\n# Create scatter plot with line of best fit (our model).\r\ngrades_hours_plot_2 <-\r\n  grades_hours_tibble %>%\r\n  ggplot(aes(x = hours_studied, y = final_grades)) +\r\n  geom_point(colour = \"blue\") +\r\n  labs(title = \"Grades vs. hours studied for Economics 101 final exam\",\r\n       x = \"Number of hours studied\",\r\n       y = \"Final exam grade (in %)\",\r\n       caption = \"Scatter plot with line of best fit.\") +\r\n  geom_smooth(method = lm, se = FALSE) +\r\n  theme_minimal()\r\ngrades_hours_plot_2\r\n\r\n\r\n\r\n\r\nWe can verify visually that our simple linear regression model was a very good fit for our data! Hope you had fun following along, and as with every blog that includes code on my website and any other section where my code has been explicitly provided or linked to, you are free to reproduce it yourself to try it out and even modify the data to try fitting models using different grades and study hours.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-28-a-lovely-day-8-for-regression-analysis/a-lovely-day-8-for-regression-analysis_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-04-28T23:48:37-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-27-wednesday-is-tidy-tuesday-on-day-7/",
    "title": "Wednesday is Tidy Tuesday on Day 7",
    "description": "My Tidy Tuesday contribution has been posted!",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-27",
    "categories": [
      "Tidy Tuesday",
      "Website updates"
    ],
    "contents": "\r\nIt may be a day late, but my Tidy Tuesday contribution has been posted! This week the data made available for the challenge was a dataset of entries for the Kaggle “Hidden Gems” notebook project, which aims to put a spotlight on notebooks that deserve more recognition than they have received. I decided to create a word cloud to visualize the most commonly used topics in the notebook titles, where the larger the word in the word cloud, the more times it was used. This required cleaning the data to only leave the relevant data, which boiled down to each unique word found in the notebook titles (except for non-significant words such as “and”, “of”, “to”, etc.) and the number of times in total that they were used.\r\nThe code for my Tidy Tuesday entry for April 26, 2022 can be found here: Code\r\nFor more information on Tidy Tuesday, click here.\r\nPicture of a word cloud depicting the most commonly referred topics in Kaggle “Hidden Gems” notebooks, where the bigger the word, the more times it was used.That’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/SimiDavid95/Tidy-Tuesdays/main/2022/Week-17/tidy_tuesday_april_26_2022.jpeg",
    "last_modified": "2022-04-29T16:01:30-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-26-day-6-website-updates/",
    "title": "Day 6 website updates on 4.26",
    "description": "New content in Educational Projects + Tidy Tuesday post delayed.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-26",
    "categories": [
      "Website updates"
    ],
    "contents": "\r\nWow, Day 6 already! Almost at the end of the first week of the website (and this blog) going live, and it does kind of feel like a blur with all the updates that have been going on. Another project has been added to the Educational Projects section of the website, and that section is starting to take shape. More projects are forthcoming, once I adapt them for public consumption and reproducibility (making sure there isn’t any sensitive data that’s being shared, for example). Other sections such as “Beyond the Data” should hopefully be updated by the time the weekend rolls around, which is ultimately my timeline for completing all the major sections of the website. Of course after that the website will continue to be updated with new content, especially with Tidy Tuesday weekly submissions. And that brings me to the final part of this post, which is that there is a delay in my Tidy Tuesday post for this week, so I should hopefully be posting it on Wednesday through my personal Twitter, and I’ll post it here as well.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-29T16:01:57-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-25-day-5-on-425/",
    "title": "Day 5 on 4.25",
    "description": "Website updates on a rainy day.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-25",
    "categories": [
      "Website updates"
    ],
    "contents": "\r\n (Screenshot by Juan David Acosta)\r\nA rainy day is such a delight to me. Seeing the raindrops fall from my study room window is a peaceful sensation. Perfect for a day of website updates! The educational projects section is beginning to take shape, and I’m making some organizational changes on my GitHub as a result. Since I was taught to put a major emphasis on reproducibility, I am making sure that the projects that I am posting here (and on my GitHub for that matter) have both the PDF report and the code file, along with any data that was used. While the data that I have accessed for my projects is open data (unless I used my own data), I will only be uploading the cleaned data to also ensure that no sensitive information is uploaded, and so this may require some modifications in the code files before they too are updated. This should all be hopefully completed in the coming days.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-25-day-5-on-425/day-5-screenshot-001.jpg",
    "last_modified": "2022-04-29T16:02:18-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-24-a-day-4-rest/",
    "title": "A Day 4 rest",
    "description": "Taking a day to recharge.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-24",
    "categories": [
      "Life",
      "Website updates"
    ],
    "contents": "\r\n (Photo by Lisa Fotios: https://www.pexels.com/photo/person-laying-on-sofa-while-reading-book-1471991/)\r\nFor Day 4 of the blog, since I took the day off from doing major updates to the website, I thought I would simply talk about my day. With beautiful weather finally arriving in Toronto today, I thought it best to take this day as one to recharge. I took in the sunlight outdoors, ate a barbecue, and shared in some de-stressing family time. Recharging truly does wonders for my mental state. It allows me time to pause and reflect on the events that have occurred up until that point, and the goals and dreams that I wish to pursue afterwards. It takes me away from the hustle and bustle, if only for a little while.\r\nNow that I am recharged, I am ready to take on the week ahead, and the various projects and goals that I have on deck. One of those projects is the website, which I plan to update extensively over the coming week, including filling in the remaining blank sections. In the words of a lovable sponge that lives in a pineapple under the sea, “I’m ready!”\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-24-a-day-4-rest/pexels-lisa-fotios-1471991.jpg",
    "last_modified": "2022-04-29T16:02:38-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-23-day-3-plot-for-me-plot-for-thee/",
    "title": "Plot for me, plot for thee on Day 3",
    "description": "Calculating summary statistics and creating a bar plot from my Wordle statistics.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-23",
    "categories": [
      "Data analysis",
      "Statistics",
      "Data visualization",
      "Wordle"
    ],
    "contents": "\r\nWordle is a game that has taken the world by storm, especially since it has offered amusement and social connection in times of pandemic. The cryptic tiles and fractions inundated countless Twitter feeds. Thus, for my Day 3 blog post, I thought it would be fun to derive some summary statistics for my Wordle wins and recreate the bar plot that Wordle provides, and for even extra fun, I will show my process throughout by posting my R code! Let’s do this!\r\nFirst, an important step is to always initialize the packages we will need for the task at hand. Since we will concern ourselves with crafting aesthetically-pleasing graphs, we will deploy the power of the Tidyverse to use the ggplot2 graphing package. Since I will be entering the data manually, there is no need for a seed to ensure reproducibility.\r\n\r\n\r\n# Load in necessary library.\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nSecond, let’s create our data. Wordle tracks statistics such as the number of games played and the guess distribution (i.e. how many games were won with one guess, two guesses, …, six guesses). These are the data that we need for our summary statistics and plot, so let’s manually input the number of games won (which in this case is the same as the number of games played) and the guess distribution.\r\n\r\n\r\n# Variable with number of games won, which is also the number of games played.\r\ngames_won <- 88\r\n\r\n# Create new variable called \"guesses\" that will store the Wordle wins based on how many guesses \r\n# it took to guess the correct word, with numerical values ranging from 2 to 6.\r\nguesses <- c(2, rep(3, 19), rep(4, 36), rep(5, 25), rep(6, 7))\r\n\r\n\r\n\r\nThird, let’s run some summary statistics on our Wordle data, such as the mean number of guesses it took to correctly guess the word of the day, the variance and standard deviation for the number of guesses, and the percentage for each of the number of guesses out of the total number of games won.\r\n\r\n\r\n# Preliminary work that groups the number of guesses in a tibble, and counts \r\n# the number of occurrences for each number of guesses.\r\ngrouped_guesses_table <- \r\n  tibble(one_guess = 0,\r\n         two_guesses = sum(guesses == 2),\r\n         three_guesses = sum(guesses == 3),\r\n         four_guesses = sum(guesses == 4),\r\n         five_guesses = sum(guesses == 5),\r\n         six_guesses = sum(guesses == 6))\r\n\r\n## Represent as kable.\r\ngrouped_guesses_kable <- \r\n  knitr::kable(grouped_guesses_table,\r\n               col.names = c(\"One guess\", \"Two guesses\", \"Three guesses\",\r\n                             \"Four guesses\", \"Five guesses\", \"Six guesses\"),\r\n               caption = \"Sum for each number of guesses.\")\r\ngrouped_guesses_kable\r\n\r\n\r\nTable 1: Sum for each number of guesses.\r\nOne guess\r\nTwo guesses\r\nThree guesses\r\nFour guesses\r\nFive guesses\r\nSix guesses\r\n0\r\n1\r\n19\r\n36\r\n25\r\n7\r\n\r\n# Percentages for each of the number of guesses out of the total number of games won.\r\npercentage_guesses_table <- \r\n  tibble(one_guess = 0,\r\n         two_guesses = (sum(guesses == 2) / games_won) * 100,\r\n         three_guesses = (sum(guesses == 3) / games_won) * 100,\r\n         four_guesses = (sum(guesses == 4) / games_won) * 100,\r\n         five_guesses = (sum(guesses == 5) / games_won) * 100,\r\n         six_guesses = (sum(guesses == 6) / games_won) * 100)\r\n\r\n## Round percentages to two decimal points in percentage_guesses_table.\r\npercentage_guesses_table <- round(percentage_guesses_table, 2)\r\n\r\n## Represent as kable.\r\npercentage_guesses_kable <- \r\n  knitr::kable(percentage_guesses_table,\r\n               col.names = c(\"One guess\", \"Two guesses\", \"Three guesses\",\r\n                             \"Four guesses\", \"Five guesses\", \"Six guesses\"),\r\n               caption = \"Percentage (%) of each number of guesses out of the total number of guesses.\")\r\npercentage_guesses_kable\r\n\r\n\r\nTable 1: Percentage (%) of each number of guesses out of the total number of guesses.\r\nOne guess\r\nTwo guesses\r\nThree guesses\r\nFour guesses\r\nFive guesses\r\nSix guesses\r\n0\r\n1.14\r\n21.59\r\n40.91\r\n28.41\r\n7.95\r\n\r\n# Calculate mean for the number of guesses, and round to two decimal places.\r\nmean_guesses <- round(mean(guesses), 2)\r\n\r\n# Calculate variance for the number of guesses, and round to two decimal places.\r\nvar_guesses <- round(var(guesses), 2)\r\n\r\n# Calculate standard deviation for the number of guesses, and round to two decimal places.\r\nsd_guesses <- round(sd(guesses), 2)\r\n\r\n# Place the mean, variance, and standard deviation in a tibble.\r\nmean_var_sd_guesses <- \r\n  tibble(mean_guesses,\r\n         var_guesses,\r\n         sd_guesses)\r\n\r\n## Represent as kable.\r\nmean_var_sd_kable <- \r\n  knitr::kable(mean_var_sd_guesses,\r\n               col.names = c(\"Mean\", \"Variance\", \"Standard deviation\"),\r\n               caption = \"Mean, variance and standard deviation for number of guesses.\")\r\nmean_var_sd_kable\r\n\r\n\r\nTable 1: Mean, variance and standard deviation for number of guesses.\r\nMean\r\nVariance\r\nStandard deviation\r\n4.2\r\n0.83\r\n0.91\r\n\r\nFourth, it’s time for the graph! We’ll create a bar graph, similar to the one that Wordle provides.\r\n\r\n\r\n# Preliminary work to make a one-column tibble from the original guesses vector.\r\nguesses_table <- tibble(guesses)\r\n\r\n# Create bar chart.\r\nbar_guesses <- guesses_table %>%\r\n  ggplot(aes(x = factor(guesses))) +\r\n  geom_bar(fill = \"#66CC33\") +\r\n  labs(title = \"Wordle Guess Distribution\",\r\n       caption = \"Created by Juan David Acosta using personal Wordle statistics.\") +\r\n  xlab(\"Number of guesses to correctly guess word\") +\r\n  ylab(\"Count\") +\r\n  theme_minimal()\r\nbar_guesses\r\n\r\n\r\n\r\n\r\nAnd there we have it! You can recreate your own Wordle guess distribution bar graph using the code above, simply updating some of the code with your own data.\r\nThat’s a wrap on today’s post!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-23-day-3-plot-for-me-plot-for-thee/day-3-plot-for-me-plot-for-thee_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-04-24T21:33:54-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-22-day-2-on-422/",
    "title": "Day 2 on 4.22",
    "description": "An Earth Day post, and more.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-22",
    "categories": [
      "Opinion",
      "Environment",
      "Website updates"
    ],
    "contents": "\r\n (Photo by Akil Mazumder: https://www.pexels.com/photo/person-holding-a-green-plant-1072824/)\r\nToday is Earth Day. A day when we reflect upon the state of our planet. With the climate crisis we are facing today, it can seem that the future is bleak, especially for the young generations that will face the brunt force of a warmer planet. However, I would like to highlight the positives, the reasons to have hope that, while we may not be able to prevent all the consequences of climate change, we can at least unite together to mitigate the most harmful outcomes. We have become more conscious of our impact on the well-being of Earth’s diverse biosphere, and are taking steps to help in any way we can to reduce that impact, whether it be buying an electric vehicle, shopping with reusable bags, planting trees, recycling, etc. It is not futile. It is not too late. Through hope and action, we can help preserve the only planet that we call home, Earth.\r\nPivoting from Earth Day, I have decided to continue posting at least one blog a day, every day! There are no guarantees for this, but I am striving to keep this up as not only an exercise in writing, but also in communicating my thoughts and ideas to the world. Thus, it will be an integral part of this website. And speaking of the website, major progress has been made in building out the rest of the website’s sections. Of course, this will be a continuing endeavour as time progresses.\r\nThat’s a wrap for this post!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-22-day-2-on-422/pexels-akil-mazumder-1072824.jpg",
    "last_modified": "2022-04-29T16:03:08-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-21-the-birth-of-a-website/",
    "title": "The birth of a website",
    "description": "From concept to reality.",
    "author": [
      {
        "name": "Juan David Acosta",
        "url": {}
      }
    ],
    "date": "2022-04-21",
    "categories": [
      "Website updates"
    ],
    "contents": "\r\n (Photo by Pixabay: https://www.pexels.com/photo/turned-on-computer-monitor-displaying-text-270360/)\r\nBuilding a website seems to be one of the fundamental basics that one must have at this point in the 21st century. A website for your business. A website for your portfolio of projects. I had attempted to make websites in the past, mostly for the times I had considered becoming a content creator, or when I fell for the allure and ease-of-use of Apple’s iWeb (this is where I date myself), but those endeavours quickly fell apart. It was a combination of no clear focus for those websites, and the amount of time that I would have to spend on them.\r\nNow, however, I have found a purpose for building a website. I am looking to break ground on my future career as a data analyst or data scientist, and it is imperative to have a central, aesthetically-pleasing looking place to highlight my projects and competencies, as well as my thoughts (like through this here blog) and ways to get in contact with me. Thus, a website is a must. And, with my intermediate knowledge of R and RStudio, what better way to put that knowledge into practice than by building a website using both R and RStudio?\r\nThus, this journey started. After consulting various articles and tutorials (that will eventually be linked here once the site is a bit more polished), the website has officially made its internet debut! There’s nothing too fancy here at the moment, but it shall soon be filled to the brim with more blog posts, projects (such as my educational projects and Tidy Tuesday submissions), biography, contact information, etc. So please, grab some snacks and drinks that I’ve set out on the table there, and take a look around!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-21-the-birth-of-a-website/pexels-pixabay-270360.jpg",
    "last_modified": "2022-04-29T16:03:48-04:00",
    "input_file": {}
  }
]
